# Pod Design

## Labels and Annotations

### Create 3 pods with names nginx1,nginx2,nginx3. All of them should have the label app=v1

<details> <summary> show </summary>

```bash
kubectl run nginx1 --image=nginx --restart=Never --labels=app=v1

kubectl run nginx2 --image=nginx --restart=Never --labels=app=v1

kubectl run nginx3 --image=nginx --restart=Never --labels=app=v1


# or,
for i in `seq 1 3` ; do kubectl run nginx$i --image=nginx --restart=Never -l app=v1 ; done
```

</details>

### Show all labels of the pods

<details> <summary> show </summary>

```bash
kubectl get po --show-labels
```

```bash
NAME     READY   STATUS    RESTARTS   AGE     LABELS
nginx1   1/1     Running   0          3m26s   app=v1
nginx2   1/1     Running   0          3m21s   app=v1
nginx3   1/1     Running   0          3m17s   app=v1
```

</details>

### Change the labels of pod 'nginx2' to be app=v2

<details> <summary> show </summary>

```bash
kubectl label po nginx2 app=v2 --overwrite
```

```bash
kubectl get po --show-labels

NAME     READY   STATUS    RESTARTS   AGE     LABELS
nginx1   1/1     Running   0          5m59s   app=v1
nginx2   1/1     Running   0          5m54s   app=v2
nginx3   1/1     Running   0          5m50s   app=v1
```

</details>

### Get the label 'app' for the pods (show a column with APP labels)

<details> <summary> show </summary>

```bash
kubectl get po -L app

or 

kubectl get po --label-columns=app

or

kubectl get po --label-columns app
```

</details>

### Get only the 'app=v2' pods

<details> <summary> show </summary>

```bash
kubectl get po -l app=v2

or

kubectl get po -l 'app in (v2)'

or

kubectl get po --selector=app=v2
```

</details>

### Add a new label tier=web to all pods having 'app=v2' or 'app=v1' labels

<details> <summary> show </summary>

```bash
kubectl label po -l "app in(v1,v2)" tier=web
```

```bash
kubectl get po --show-labels 

NAME     READY   STATUS    RESTARTS   AGE   LABELS
nginx1   1/1     Running   0          14m   app=v1,tier=web
nginx2   1/1     Running   0          13m   app=v2,tier=web
nginx3   1/1     Running   0          13m   app=v1,tier=web
```

</details>

### Add an annotation 'owner: marketing' to all pods having 'app=v2' label

<details> <summary> show </summary>

```bash
kubectl annotate po -l app=v2 owner=marketing
```

```bash
kubectl get po -l app=v2

NAME     READY   STATUS    RESTARTS   AGE
nginx2   1/1     Running   0          17m
```

```bash
Name:             nginx2
Namespace:        default
Priority:         0
Service Account:  default
Node:             minikube/192.168.49.2
Start Time:       Sat, 25 Mar 2023 09:45:55 +0530
Labels:           app=v2
                  tier=web
Annotations:      owner: marketing
Status:           Running
IP:               172.17.0.4
IPs:
  IP:  172.17.0.4
Containers:
  nginx2:
    Container ID:   docker://df2f191b8d0c5a1dc386c8d3eb8d9ce61907a4f588f4612dd462fd2799562c2d
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:f4e3b6489888647ce1834b601c6c06b9f8c03dee6e097e13ed3e28c01ea3ac8c
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sat, 25 Mar 2023 09:46:00 +0530
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tzzx7 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-tzzx7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  17m   default-scheduler  Successfully assigned default/nginx2 to minikube
  Normal  Pulling    17m   kubelet            Pulling image "nginx"
  Normal  Pulled     17m   kubelet            Successfully pulled image "nginx" in 4.308765496s
  Normal  Created    17m   kubelet            Created container nginx2
  Normal  Started    17m   kubelet            Started container nginx2
```

</details>

### Remove the 'app' label from the pods we created before

<details> <summary> show </summary>

```bash
kubectl label po nginx1 nginx2 nginx3 app-
# or
kubectl label po nginx{1..3} app-
# or
kubectl label po -l app app-
```

</details>


### Create a pod that will be deployed to a Node that has the label 'accelerator=nvidia-tesla-p100'

<details> <summary> show </summary>

Before new node selector add
```bash
kubectl get nodes --show-labels

NAME       STATUS   ROLES           AGE   VERSION   LABELS
minikube   Ready    control-plane   34m   v1.24.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=62e108c3dfdec8029a890ad6d8ef96b6461426dc,minikube.k8s.io/name=minikube,minikube.k8s.io/primary=true,minikube.k8s.io/updated_at=2023_03_25T09_35_59_0700,minikube.k8s.io/version=v1.26.1,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
```

New node selctor add
```bash
kubectl label nodes minikube accelerator=nvidia-tesla-p100
```

After new node selector add
```bash
kubectl get nodes --show-labels

NAME       STATUS   ROLES           AGE   VERSION   LABELS
minikube   Ready    control-plane   35m   v1.24.3   accelerator=nvidia-tesla-p100,beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=62e108c3dfdec8029a890ad6d8ef96b6461426dc,minikube.k8s.io/name=minikube,minikube.k8s.io/primary=true,minikube.k8s.io/updated_at=2023_03_25T09_35_59_0700,minikube.k8s.io/version=v1.26.1,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=
```

create new pod
```bash
kubectl run nginx4 --image=nginx --restart=Never --dry-run=client -o yaml > node-selector.yaml
```

Update pod to schedule pod in specific node
```bash
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx4
  name: nginx4
spec:
  containers:
  - image: nginx
    name: nginx4
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  
  nodeSelector:
    accelerator: nvidia-tesla-p100
status: {}
```

or using node affinity
```bash
apiVersion: v1
kind: Pod
metadata:
  name: affinity-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: accelerator
            operator: In
            values:
            - nvidia-tesla-p100
  containers:
  - image: nginx
    name: nginx4
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
```

</details>

### Annotate pods nginx1, nginx2, nginx3 with "description='my description'" value

<details> <summary> show </summary>

```bash
kubectl annotate po nginx1 nginx2 nginx3 description='my description'

#or

kubectl annotate po nginx{1..3} description='my description'
```

</details>

### Check the annotations for pod nginx1

```bash

kubectl get po nginx1 -o 'jsonpath={.metadata.annotations}{"\n"}'

# or

kubectl annotate pod nginx1 --list

# or

kubectl describe po nginx1 | grep -i 'annotations'

#or

kubectl get po nginx1 -o custom-columns=Name:metadata.name,ANNOTATIONS:metadata.annotations
```

### Remove the annotation 'description' for these three pods

``` bash
kubectl annotate po nginx{1..3} description-
```

### Remove these pods to have a clean state in your cluster

```bash
kubectl delete po nginx{1..3}
```

## Deployments

### Create a deployment with image nginx:1.18.0, called nginx, having 2 replicas, defining port 80 as the port that this container exposes (don't create a service for this deployment)

<details> <summary> show </summary>

```bash
kubectl create deployment nginx --image=nginx:1.18.0 --dry-run=client -o yaml > nginx-deploy.yaml
```

update deployment to change replicas from 1 to 2 and add a ports: -containerPort: 80 to spec section of the container
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.18.0
        name: nginx
        ports:
	  - containerPort: 80
        resources: {}
status: {}
```

apply the deployment
```bash
kubectl apply -f deploy.yaml
```

or 

```bash
kubectl create deployment nginx  --image=nginx:1.18.0  --dry-run=client -o yaml | sed 's/replicas: 1/replicas: 2/g'  | sed 's/image: nginx:1.18.0/image: nginx:1.18.0\n        ports:\n        - containerPort: 80/g' | kubectl apply -f -
```

or 

```bash
kubectl create deploy nginx --image=nginx:1.18.0 --replicas=2 --port=80
```
</details>

### View the YAML of this deployment

<details> <summary> show </summary>

```bash
kubectl get deploy nginx -o yaml
```

</details>

### View the YAML of the replica set that was created by this deployment

<details> <summary> show </summary> 

```bash
kubectl describe deploy nginx # you'll see the name of the replica set on the Events section and in the 'NewReplicaSet' property

# OR you can find rs directly by:

kubectl get rs -l run=nginx # if you created deployment by 'run' command
kubectl get rs -l app=nginx # if you created deployment by 'create' command

# you could also just do kubectl get rs

kubectl get rs nginx-7bf7478b77 -o yaml
```

</details>

### Get the YAML for one of the pods

<details> <summary> show </summary> 

```bash
kubectl get po -l run=nginx # if you created deployment by 'run' command
kubectl get po -l app=nginx # if you created deployment by 'create' command
```

```bash
kubectl get po nginx-6dc6fccf5-5mk86 -o yaml
```

</details>

### Check how the deployment rollout is going

<details> <summary> show </summary> 

```bash
kubectl rollout status deploy nginx
```

</details>


### Update the nginx image to nginx:1.19.8

<details> <summary> show </summary> 

```bash
kubectl set image deploy nginx nginx=nginx:1.19.8
```

</details>

### Check the rollout history and confirm that the replicas are OK

<details> <summary> show </summary> 

```bash
kubectl rollout history deploy nginx
```

</details>

### Undo the latest rollout and verify that new pods have the old image (nginx:1.18.0)

<details> <summary> show </summary> 

```bash
kubectl rollout undo deploy nginx

kubectl get po -l run=nginx # if you created deployment by 'run' command
kubectl get po -l app=nginx # if you created deployment by 'create' command

kubectl describe po nginx-6dc6fccf5-6v4qx
```

</details>

### Do an on purpose update of the deployment with a wrong image nginx:1.91

<details> <summary> show </summary> 

```bash
kubectl set image deploy nginx nginx=nginx:1.91
```

</details>

### Verify that something's wrong with the rollout

<details> <summary> show </summary> 

```bash
kubectl rollout status deploy nginx
```

</details>

### Return the deployment to the second revision (number 2) and verify the image is nginx:1.19.8

<details> <summary> show </summary> 

```bash
kubectl rollout undo deploy nginx --to-revision=2
```

</details>

### Check the details of the fourth revision (number 4)

<details> <summary> show </summary> 

kubectl rollout history deployment nginx --revision=4

</details>

### Scale the deployment to 5 replicas

<details> <summary> show </summary> 

kubectl scale deploy nginx --replicas=5

</details>

### Autoscale the deployment, pods between 5 and 10, targetting CPU utilization at 80%

<details> <summary> show </summary> 

```bash
kubectl autoscale deployment nginx --min=5 --max=10 --cpu-percent=80
kubectl get hpa nginx
```

</details>

### Pause the rollout of the deployment

<details> <summary> show </summary> 

```bash
kubectl rollout pause deploy ngnix
```

</details>

### Update the image to nginx:1.19.9 and check that there's nothing going on, since we paused the rollout

<details> <summary> show </summary> 

```bash
kubectl set image deploy nginx nginx=nginx:1.19.9

kubectl rollout history deploy nginx
```

</details>

### Resume the rollout and check that the nginx:1.19.9 image has been applied

<details> <summary> show </summary> 

```bash
kubectl rollout pause deploy ngnix

kubectl get rs
kubectl rollout history deploy nginx
kubectl rollout history deploy nginx --revision=6 
```

</details>

### Delete the deployment and the horizontal pod autoscaler you created

<details> <summary> show </summary> 

```bash
kubectl delete deployments nginx
kubectl delete hpa nginx
```

</details>

### Implement canary deployment by running two instances of nginx marked as version=v1 and version=v2 so that the load is balanced at 75%-25% ratio

<details> <summary> show </summary> 

Deploy 3 replicas of v1:
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-v1
  labels:
    app: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
      version: v1
  template:
    metadata:
      labels:
        app: my-app
        version: v1
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      initContainers:
      - name: install
        image: busybox:1.28
        command:
        - /bin/sh
        - -c
        - "echo version-1 > /work-dir/index.html"
        volumeMounts:
        - name: workdir
          mountPath: "/work-dir"
      volumes:
      - name: workdir
        emptyDir: {}
```

Create the service:
```bash
apiVersion: v1
kind: Service
metadata:
  name: my-app-svc
  labels:
    app: my-app
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: my-app
```


Test if the deployment was successful from within a Pod:
```bash
kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox --command -- wget -qO- my-app-svc
```

Deploy 1 replica of v2:
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app-v2
  labels:
    app: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
      version: v2
  template:
    metadata:
      labels:
        app: my-app
        version: v2
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
      initContainers:
      - name: install
        image: busybox:1.28
        command:
        - /bin/sh
        - -c
        - "echo version-2 > /work-dir/index.html"
        volumeMounts:
        - name: workdir
          mountPath: "/work-dir"
      volumes:
      - name: workdir
        emptyDir: {}
```

Make calls and check load distribution:
```bash
kubectl run -it --rm --restart=Never busybox --image=gcr.io/google-containers/busybox -- /bin/sh -c 'while sleep 1; do wget -qO- my-app-svc; done'
```

If the v2 is stable, scale it up to 4 replicas and shoutdown the v1:
```bash
kubectl scale --replicas=4 deploy my-app-v2
kubectl delete deploy my-app-v1

while sleep 0.1; do curl $(kubectl get svc my-app-svc -o jsonpath="{.spec.clusterIP}"); done
```

</details>