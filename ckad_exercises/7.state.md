## State

### Volumes

<details> <summary> show </summary>

```bash
apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
  - name: redis
    image: redis
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
```

In the above `Pod` manifest, an `emptyDir` volume `redis-storage` is created and that volume is mounted to one of the container `redis` in the pod.

apply the above manifest
```bash
kubectl apply -f redis-manifest.yaml
```

check pod status, wait for it to be running
```bash
watch -n 10 kubectl get pod
<<com
Every 10.0s: kubectl get po                                                                                                                                     hitesh-pattanayak: Sat Apr 22 09:24:17 2023

NAME                                    READY   STATUS    RESTARTS   AGE
redis                                   1/1     Running   0          2m5s
com
```

get into redis pod
```bash
kubectl exec -it redis -- bash
root@redis:/data# ls /data 
redis
root@redis:/data# ls /data/redis/
root@redis:/data# echo Hello > /data/redis/hello.txt
root@redis:/data# ls /data/redis/
hello.txt
root@redis:/data# cat /data/redis/hello.txt 
Hello

# in another terminal do below
kubectl get po --watch

# continue in previous terminal where you are within pod
root@redis:/data#  apt-get update
root@redis:/data# apt-get install procps
root@redis:/data# ps aux
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
redis          1  0.1  0.0  53692  8440 ?        Ssl  03:58   0:00 redis-server *:6379
root          29  0.0  0.0   4164  3328 pts/0    Ss   04:01   0:00 bash
root         362  0.0  0.0   6760  2852 pts/0    R+   04:04   0:00 ps aux
root@redis:/data# kill 1
root@redis:/data# command terminated with exit code 137

# in the other terminal, notice this
kubectl get po --watch
NAME                                    READY   STATUS    RESTARTS   AGE
redis                                   1/1     Running   0          4m4s
redis                                   0/1     Completed   0          6m25s
redis                                   1/1     Running     1 (6s ago)   6m30s


# exec into the pod again
kubectl exec -it redis -- bash
root@redis:/data# ls
redis
root@redis:/data# ls redis/
hello.txt
root@redis:/data# cat redis/hello.txt 
Hello
root@redis:/data# exit
exit
```

the volume persisted even when container within pod went down.

delete the pod, apply the pod again, exec into the pod and look for hello.txt file in /data/redis
```bash
kubectl delete po redis
kubectl apply -f redis-manifest.yaml
kubectl exec -it redis -- bash
root@redis:/data# ls
redis
root@redis:/data# cd redis/
root@redis:/data/redis# ls
root@redis:/data/redis# exit
exit
```

you could not find the file as the volume is not persistent when pod goes down. This is an ephemeral volume tied up with pod lifecycle.

</details>

### Persistent Volumes and Persistent Volume Claims

<details> <summary> show </summary>

ssh into your node and create an index.html file in a directory
```bash
minikube ssh
Last login: Thu Apr 13 17:49:49 2023 from 192.168.49.1
docker@minikube:~$ sudo mkdir /mnt/data
docker@minikube:~$ ls /mnt/data/
docker@minikube:~$ sudo sh -c "echo 'Hello from Kubernetes storage' > /mnt/data/index.html"
docker@minikube:~$ ls /mnt/data/
index.html
docker@minikube:~$ cat /mnt/data/index.html 
Hello from Kubernetes storage
docker@minikube:~$ exit
logout
```

create a persistent volume manifest file
```bash
touch pv.yaml

nano pv.yaml

# update the file with below content
<<com
apiVersion: v1
kind: PersistentVolume
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
com

kubectl apply -f pv.yaml  # persistentvolume/task-pv-volume created

kubectl get pv

<<com
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Available           manual                  7s
com
```

Notice the `Status` is `Available`

Now lets create a claim for above persistent volume. Keep the `storageClassName` same and storage limits within persistent volume limits.
```bash
touch pvc.yaml

nano pvc.yaml
# update the file with below content
<<com
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
com

kubectl apply -f pvc.yaml

# Recheck the previous persistent volume status
kubectl get pv task-pv-volume
<<com
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS   REASON   AGE
task-pv-volume   10Gi       RWO            Retain           Bound    default/task-pv-claim   manual                  3m40s
com

kubectl get pvc task-pv-claim
<<com
NAME            STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE
task-pv-claim   Bound    task-pv-volume   10Gi       RWO            manual         66s
com
```

Lets create a pod and associate it with above claim

```bash
kubectl run pvc-nginx --image=nginx --dry-run=client -o yaml > pvc-nginx-pod.yaml

# update the pod manifest to below
<<com
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvc-nginx
  name: pvc-nginx
spec:
  volumes:
  - name: vol1
    persistentVolumeClaim:
      claimName: task-pv-claim
  containers:
  - image: nginx
    name: pvc-nginx
    ports:
    - containerPort: 80
    volumeMounts:
    - name: vol1
      mountPath: "/usr/share/nginx/html"
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
com

kubectl apply -f pvc-nginx-pod.yaml

kubectl get po -l run=pvc-nginx -o wide 

<<com
NAME        READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
pvc-nginx   1/1     Running   0          33s   172.17.0.3   minikube   <none>           <none>
com

kubectl exec -it pvc-nginx -- bash
root@pvc-nginx:/# curl localhost:80
Hello from Kubernetes storage
root@pvc-nginx:/# exit
exit
```

cleanup
```bash
kubectl delete pod pvc-nginx
kubectl delete pvc task-pv-claim
kubectl delete pv task-pv-volume

minikube ssh
Last login: Sat Apr 22 07:34:00 2023 from 192.168.49.1
docker@minikube:~$ ls /mnt/data/
index.html
docker@minikube:~$ sudo rm -r /mnt/data/
docker@minikube:~$ exit
logout
```

can also mount same persistent volume in 2 places
```bash
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
    - name: test
      image: nginx
      volumeMounts:
        # a mount for site-data
        - name: config
          mountPath: /usr/share/nginx/html
          subPath: html
        # another mount for nginx config
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
  volumes:
    - name: config
      persistentVolumeClaim:
        claimName: test-nfs-claim
```

in order to control access to persistent volume annotate PV, PVC and Pods with same GID for better management.

```bash
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv1
  annotations:
    pv.beta.kubernetes.io/gid: "1234"
```

</details>

### Create busybox pod with two containers, each one will have the image busybox and will run the 'sleep 3600' command. Make both containers mount an emptyDir at '/etc/foo'. Connect to the second busybox, write the first column of '/etc/passwd' file to '/etc/foo/passwd'. Connect to the first busybox and write '/etc/foo/passwd' file to standard output. Delete pod.

<details> <summary> show </summary>

```bash
kubectl run busybox --image=busybox --restart=Never -o yaml --dry-run=client -- /bin/sh -c 'sleep 3600' > state-busybox-pod.yaml

#update the state-busybox-pod.yaml to below
<<com
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  dnsPolicy: ClusterFirst
  restartPolicy: Never
  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}
    volumeMounts: #
    - name: myvolume #
      mountPath: /etc/foo #
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: busybox2 # don't forget to change the name during copy paste, must be different from the first container's name!
    volumeMounts: #
    - name: myvolume #
      mountPath: /etc/foo #
  volumes: #
  - name: myvolume #
    emptyDir: {} #
com

kubectl apply -f state-busybox-pod.yaml

kubectl get po -l "run=busybox"
<<com
NAME      READY   STATUS    RESTARTS   AGE
busybox   2/2     Running   0          20s
com

kubectl exec -it busybox -c busybox2 -- /bin/sh
/ # 
/ # cat /etc/passwd 
root:x:0:0:root:/root:/bin/sh
daemon:x:1:1:daemon:/usr/sbin:/bin/false
bin:x:2:2:bin:/bin:/bin/false
sys:x:3:3:sys:/dev:/bin/false
sync:x:4:100:sync:/bin:/bin/sync
mail:x:8:8:mail:/var/spool/mail:/bin/false
www-data:x:33:33:www-data:/var/www:/bin/false
operator:x:37:37:Operator:/var:/bin/false
nobody:x:65534:65534:nobody:/home:/bin/false
/ # cat /etc/passwd | cut -f 1 -d ':'
root
daemon
bin
sys
sync
mail
www-data
operator
nobody
/ # cat /etc/passwd | cut -f 1 -d ':' > /etc/foo/passwd
/ # cat /etc/foo/passwd 
root
daemon
bin
sys
sync
mail
www-data
operator
nobody
/ # exit

kubectl exec -it busybox -c busybox -- /bin/sh
kubectl exec -it busybox -c busybox -- /bin/sh
/ # 
/ # cat /etc/foo/passwd 
root
daemon
bin
sys
sync
mail
www-data
operator
nobody
/ # exit

kubectl delete po busybox
```

</details>

### Create a PersistentVolume of 10Gi, called 'myvolume'. Make it have accessMode of 'ReadWriteOnce' and 'ReadWriteMany', storageClassName 'normal', mounted on hostPath '/etc/foo'. Save it on pv.yaml, add it to the cluster. Show the PersistentVolumes that exist on the cluster

<details> <summary> show </summary>

```bash
minikube ssh
Last login: Sat Apr 22 07:52:30 2023 from 192.168.49.1
docker@minikube:~$ sudo mkdir /etc/foo
docker@minikube:~$ ls /etc/foo
docker@minikube:~$ exit
logout


touch myvolume.yaml

nano myvolume.yaml
# update the file to below contents
<<com
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvolume
  labels:
    type: local
spec:
  storageClassName: normal
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
    - ReadWriteMany
  hostPath:
    path: "/etc/foo"
com

kubectl apply -f myvolume.yaml # persistentvolume/myvolume created

kubectl get pv 
<<com
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
myvolume   10Gi       RWO,RWX        Retain           Available           normal                  17s
com
```
</details>

### Create a PersistentVolumeClaim for this storage class, called 'mypvc', a request of 4Gi and an accessMode of ReadWriteOnce, with the storageClassName of normal, and save it on pvc.yaml. Create it on the cluster. Show the PersistentVolumeClaims of the cluster. Show the PersistentVolumes of the cluster

<details> <summary> show </summary>

```bash
touch mypvc.yaml

nano mypvc.yaml

# update the file to below contents
<<com
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
spec:
  storageClassName: normal
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi
com

kubectl apply -f mypvc.yaml

kubectl get pvc
<<com
kubectl get pvc
NAME    STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mypvc   Bound    myvolume   10Gi       RWO,RWX        normal         5s
com 

kubectl get pv
<<com
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM           STORAGECLASS   REASON   AGE
myvolume   10Gi       RWO,RWX        Retain           Bound    default/mypvc   normal                  3m18s
com
```

</details>

### Create a busybox pod with command 'sleep 3600', save it on pod.yaml. Mount the PersistentVolumeClaim to '/etc/foo'. Connect to the 'busybox' pod, and copy the '/etc/passwd' file to '/etc/foo/passwd'

<details> <summary> show </summary>

```bash
kubectl run volumes-busybox --image=busybox --dry-run=client -o yaml -- /bin/sh -c "sleep 3600" > volumes-busybox-pod.yaml

# update pod manifest to below
<<com
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: volumes-busybox
  name: volumes-busybox
spec:
  volumes:
  - name: vol1
    persistentVolumeClaim:
      claimName: mypvc
  containers:
  - args:
    - /bin/sh
    - -c
    - sleep 3600
    image: busybox
    name: volumes-busybox
    volumeMounts:
    - name: vol1
      mountPath: /etc/foo
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
com

kubectl apply -f volumes-busybox-pod.yaml  # pod/volumes-busybox created

kubectl exec volumes-busybox -it -- cp /etc/passwd /etc/foo/passwd
```

</details>

### Create a second pod which is identical with the one you just created (you can easily do it by changing the 'name' property on pod.yaml). Connect to it and verify that '/etc/foo' contains the 'passwd' file. Delete pods to cleanup. Note: If you can't see the file from the second pod, can you figure out why? What would you do to fix that?


<details> <summary> show </summary>

```bash
cp volumes-busybox-pod.yaml volumes-busybox-pod-2.yaml

nano volumes-busybox-pod-2.yaml

# update name fields 

kubectl apply -f volumes-busybox-pod-2.yaml

kubectl exec volumes-busybox-2 -- ls /etc/foo # passwd

# cleanup

kubectl delete po volumes-busybox volumes-busybox-2 
#pod "volumes-busybox" deleted
#pod "volumes-busybox-2" deleted

kubectl delete pv myvolume # persistentvolume "myvolume" deleted
kubectl delete pvc mypvc  #persistentvolumeclaim "mypvc" deleted
```

If the file doesn't show on the second pod but it shows on the first, it has most likely been scheduled on a different node

check nodes
```bash
kubectl get po busybox -o wide
kubectl get po busybox2 -o wide
```

</details>

### Create a busybox pod with 'sleep 3600' as arguments. Copy '/etc/passwd' from the pod to your local folder

<details> <summary> show </summary>

```bash
kubectl run busybox --image=busybox --restart=Never -- sleep 3600
kubectl cp busybox:/etc/passwd ./passwd
cat passwd
```

</details>