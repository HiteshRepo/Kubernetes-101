## Custom Resource Definition & Custom Controllers

Lets consider 'Deployment' resource.

When we create a deployment using `kubectl create`, it creates a deployment entry in the `etcd`.

Any other commands like `kubectl delete` / `kubectl apply` will do its operation on `etcd`.

But the real work of creating pods based on replica count, container images, etc is done by `Deployment Controller`.

`Deployment Controllers` are built in as many other controllers. These controllers watch `etcd` about the resources they are supposed to manage and do operations based on the desired status in the `etcd`.

So basically the kubernetes operates on combination of `Resource` and `Controller`.

Custom Resource Definition can be created in order to make kubernetes understand about a new kind of resource. But the actual work can only be done by implementing a custom controller for that partcular CRD.

### Let consider a new Resource called 'FlightTicket'

```bash
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2
```

Lets create the above 'FlightTicket' resource.

```bash
kubectl create -f flightticket.yaml

## no matches for 'kind' FlightTicket in version 'flights.com/v1'
```

### Make kubernetes understand about new Resource called 'FlightTicket'

```bash
apiVersion: apiextension.k8s.io
kind: CustomResourceDefinition
metadata:
  name: flighttickets.flights.com
spec:
  scope: Namespaced
  group: flights.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                from:
                  type: string
                to:
                  type: string
                number:
                  type: integer
                  minimum: 1
                  maximum: 10
  names:
    kind: FlightTicket
    singular: flightticket
    plural: flighttickets
    shortNames:
      - ft
  
```

### Creating a K8scontroller from scratch

*** main.go ***

create a config object using kube-config file location
if fails to build config from file, fetch in cluster config

```go
import (
  "k8s.io/client-go/tools/clientcmd"
  "k8s.io/client-go/rest"
  "k8s.io/client-go/kubernetes"
  "k8s.io/client-go/informers"
)

kubeconfig := flag.String("kubeconfig", "/home/hitesh/.kube/config", "location of your kubeconfig file")
config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
if err != nil {
  fmt.Printf("error while building config from kubeconfig file location")
  config, err = rest.InClusterConfig()
  if err != nil {
    fmt.Printf("error while getting inclusterconfig")
    return
  }
}
```

create a `clientSet` object using `config`.
`clientSet` is created to perform various operations on the Kubernetes cluster, such as creating, updating, and deleting resources.

```go
clientSet, err := kubernetes.NewForConfig(config)
if err != nil {
  fmt.Println("error while creating client")
  return
}
```

create an `informer` object using `clientSet` and a resync period time interval.
An `informer` is a powerful mechanism in the Kubernetes client-go library that provides a way for clients to efficiently watch for changes to Kubernetes resources. Informers use a cache to store a local copy of the resources they are watching, and then provide event notifications whenever a change is detected. This mechanism helps clients to avoid unnecessary API server calls and to efficiently handle updates to resources.

```go
informers, err := informers.NewSharedInformerFactory(clientSet, 10*time.Minute)
if err != nil {
  fmt.Println("error while getting informer factory")
  return
}
```

create an object of controller by passing a channel the controller is expecting.
start the informer before running the controller.

```go
ch := make(chan struct{})

c := newController(clientSet, infromers.Apps().V1().Deployments())
informers.Start(ch)
c.run(ch)
```

*** controller.go ***


```go
import (
  "k8s.io/client-go/kubernetes"
  appListers "k8s.io/client-go/listers/apps/v1"
  "k8s.io/client-go/tools/cache"
  "k8s.io/client-go/util/workqueue"
  appInformers "k8s.io/client-go/informers/apps/v1"
)

type controller struct {
  clientSet kubernetes.Interface
  depLister appListers.DeploymentLister
  depCachedSync cache.InfromerSynced
  queue workqueue.RateLimitingInterface
}

func newController(clientSet kubernetes.Interface, depInformer appInformers.DeploymentInformer) *controller {
  c := &controller{
    clientset: clientSet,
    depLister: depInformer.Lister()
    depCachedSync: depInformer.Informer.HasSynced,
    queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), "queue-name")
  }

  depInformer.Informer().AddEventHandler(
    cache.ResourceEventHandlerFuncs{
      AddFunc: c.handleAdd,
      DeleteFunc: c.handleDel,
    },
  )

  return c
}

func (c *controller) run(ch <-chan struct{}) {
  fmt.Println("starting controller")
  if !cache.WaitForCacheSync(ch, c.depCachedSync) {
    fmt.Println("waiting for cache to be synced")
  }

  wait.Until(c.worker, 1 * time.Second, c)

  <-ch
}

func (c *controller) worker() {
  for c.processItem() {

  }
}

func (c *controller) processItem() bool {
  item, shutdown := c.queue.Get()
  if shutdown {
    return false
  }

  key, err := cache.MetaNamespaceKeyFunc(item)
  if err != nil {
    fmt.Printf("error while getting key from cache")
    return false
  }

  ns, name, err := cache.SplitMetaNamespaceKey(key)
  if err != nil {
    fmt.Printf("error while splitting key into namespace and name")
    return false
  }

  err = c.syncDeployment()
  if err != nil {
    fmt.Printf("error while syncing deployment")
    return false
  }

  return true
}

func (c *controller) syncDeployment(ns, name string) error {
  // do stuff

  return nil
}

func (c *controller) handleAdd(obj interface{}) {
  fmt.Println("add was called")
  c.queue.Add(obj)
}

func (c *controller) handleDel(obj interface{}) {
  fmt.Println("del was called")
  c.queue.Add(obj)
}

```